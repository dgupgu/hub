{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUymE2l9GZfO"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "code",
    "id": "JMyTNwSJGGWg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co7MV6sX7Xto"
   },
   "source": [
    "# Universal Sentence Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/s?q=google%2Funiversal-sentence-encoder%2F4%20OR%20google%2Funiversal-sentence-encoder-large%2F5\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAVQGidpL8v5"
   },
   "source": [
    "This notebook illustrates how to access the Universal Sentence Encoder and use it for sentence similarity and sentence classification tasks.\n",
    "\n",
    "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOTzp8O36CyQ"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lVjNK8shFKOC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63Pd3nJnTl-i"
   },
   "source": [
    "More detailed information about installing Tensorflow can be found at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x1468e71f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x1468e71f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#ran correctly with ipython. issue with jupyter notebook environment.\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8F4LNGFqOiq"
   },
   "outputs": [],
   "source": [
    "#@title Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "message_embeddings = embed(messages)\n",
    "\n",
    "#message_embeddings is a list of embeddings\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(message_embedding[i])\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "      (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Compute a representation for each message, showing various lengths supported.\n",
    "sentence1 = \"I am Darren, and I like to eat bananas.\"\n",
    "sentence2 = \"Monkeys chomp on bananas for fun.\"\n",
    "sentence3 = \"Darren likes to play with monkeys during his free time.\"\n",
    "messages = [sentence1, sentence2, sentence3]\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "message_embeddings = embed(messages)\n",
    "\n",
    "#message_embeddings is a list of embeddings\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(message_embedding[i])\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "      (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnvjATdy64eR"
   },
   "source": [
    "# Semantic Textual Similarity Task Example\n",
    "\n",
    "The embeddings produced by the Universal Sentence Encoder are approximately normalized. The semantic similarity of two sentences can be trivially computed as the inner product of the encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1FFCTKm7ba4"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55349964/how-to-use-pandas-rolling-mean-for-3d-input-array\n",
    "#https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "#https://www.geeksforgeeks.org/python-using-2d-arrays-lists-the-right-way/ (method 2a)\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "    message_embeddings_ = embed(messages_)\n",
    "    #plot_similarity(messages_, message_embeddings_, 90)\n",
    "    #print(message_embeddings_)\n",
    "    \n",
    "    #first, create arrays for each embed_dimension. (512 arrays, each with size 76)\n",
    "    rows, cols = (512, 76) \n",
    "    arr = [[0]*cols]*rows \n",
    "    \n",
    "    #for each sentence (76). i ranges from 0 to 76\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings_).tolist()):\n",
    "       #add the sentence's different embed_dimensions to each array for embed_dimension\n",
    "        for row in arr:\n",
    "            for j in range(512):\n",
    "                #message_embedding is at the level of each sentence\n",
    "                row[j]=message_embedding[j]\n",
    "    \n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(message_embeddings_)=76 words. len(message_embeddings_[0]=512 indices)   \n",
    "#message embeddings have all the words, as arrays of 512 indices. \n",
    "# for i in range(len(message_embeddings_[0])):\n",
    "#         prefix = \"dimension:\"\n",
    "#         dimension = prefix+str(i)\n",
    "#         dimension=[]\n",
    "#         print(dimension)\n",
    "#         for j, message_embedding in enumerate(np.array(message_embeddings_).tolist()):\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "339tuJ5Pwqqv"
   },
   "source": [
    "## Similarity Visualized\n",
    "Here we show the similarity in a heat map. The final graph is a 9x9 matrix where each entry `[i, j]` is colored based on the inner product of the encodings for sentence `i` and `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "annotations = pd.read_csv(\"~/Projects/CDL/perceptual-dynamics/data/annotations/moneybox.csv\")\n",
    "transcript=annotations[\"Narrative details (external events)\"]\n",
    "messages = list(transcript)\n",
    "embeddings=run_and_plot(messages)\n",
    "\n",
    "#df=pd.DataFrame(messages)\n",
    "#df.rolling(2, win_type='gaussian').sum(std=3)\n",
    "#messages = list(df)\n",
    "#embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = list(transcript[\"Narrative details (external events)\"][2:8])\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = list(transcript[\"Narrative details (external events)\"][15:41])\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = list(transcript[\"Narrative details (external events)\"][49:60])\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = list(transcript[\"Narrative details (external events)\"][69:76])\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = open(\"/Users/darrengu/Projects/CDL/perceptual-dynamics/code/Darren_Sherlock/raw/P1.txt\", \"r\")\n",
    "P2 = open(\"/Users/darrengu/Projects/CDL/perceptual-dynamics/code/Darren_Sherlock/raw/P2.txt\", \"r\")\n",
    "\n",
    "messages=[P1.read(), P2.read()]\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(\"~/Projects/CDL/perceptual-dynamics/code/Darren_Sherlock/raw/P1_sentences.csv\")\n",
    "messages = list(p1[\"Narrative details (external events)\"])\n",
    "embeddings=run_and_plot(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = pd.read_csv(\"~/Projects/CDL/perceptual-dynamics/code/Darren_Sherlock/raw/P2_sentences.csv\")\n",
    "messages = list(p2[\"Narrative details (external events)\"])\n",
    "embeddings=run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPMCaxrZwp7t"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]\n",
    "\n",
    "run_and_plot(messages)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FjdeCqPJeg-"
   },
   "source": [
    "## Evaluation: STS (Semantic Textual Similarity) Benchmark\n",
    "\n",
    "The [**STS Benchmark**](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is then used to evaluate the quality of the machine similarity scores against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5nuBbI1iFQR"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOs8ZfOnJeBF"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import scipy\n",
    "import math\n",
    "import csv\n",
    "\n",
    "sts_dataset = tf.keras.utils.get_file(\n",
    "    fname=\"Stsbenchmark.tar.gz\",\n",
    "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
    "    extract=True)\n",
    "sts_dev = pandas.read_table(\n",
    "    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n",
    "    error_bad_lines=False,\n",
    "    skip_blank_lines=True,\n",
    "    usecols=[4, 5, 6],\n",
    "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
    "sts_test = pandas.read_table(\n",
    "    os.path.join(\n",
    "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
    "    error_bad_lines=False,\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    skip_blank_lines=True,\n",
    "    usecols=[4, 5, 6],\n",
    "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
    "# cleanup some NaN values in sts_dev\n",
    "sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OKy8WhnKRe_"
   },
   "source": [
    "### Evaluate Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-q2r7jyZGb7"
   },
   "outputs": [],
   "source": [
    "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
    "\n",
    "def run_sts_benchmark(batch):\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n",
    "    cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "    scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
    "    \"\"\"Returns the similarity scores\"\"\"\n",
    "    return scores\n",
    "\n",
    "dev_scores = sts_data['sim'].tolist()\n",
    "scores = []\n",
    "for batch in np.array_split(sts_data, 10):\n",
    "    scores.extend(run_sts_benchmark(batch))\n",
    "\n",
    "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
    "    pearson_correlation[0], pearson_correlation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RUymE2l9GZfO"
   ],
   "name": "Semantic Similarity with TF-Hub Universal Encoder",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
